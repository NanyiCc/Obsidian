Tokenizer 和 Embedding 是自然语言处理（NLP）中的两个基本步骤，它们分别对应于文本预处理和特征提取的过程。

1. **Tokenizer**：Tokenizer 的主要任务是将原始的文本数据转化为可以供模型处理的数值化的形式。这通常涉及到将句子分割为单词（或更小的单位，如子词或字符），并将这些单词转换为对应的整数索引。这个过程称为“tokenization”。例如，句子 "I love AI" 可能会被分割为三个单词 "I", "love", "AI"，并分别转化为三个整数，如 1, 2, 3。这样，模型就可以处理这些整数，而不是直接处理文本数据。

2. **Embedding**：Embedding 是将这些整数索引（也称为 tokens）映射到高维向量空间中的过程。这个向量空间通常被称为"embedding space"。在这个空间中，语义相近的单词会被映射到相近的位置。例如，"king" 和 "queen" 这两个单词在向量空间中的位置会比 "king" 和 "apple" 更接近。这个过程通常是通过预训练的词嵌入模型（如 Word2Vec、GloVe 或 Transformer 类模型如 BERT、GPT 等）实现的。

总的来说，Tokenizer 负责将原始的文本数据转化为一种可以供模型处理的数值化形式，而 Embedding 则是负责将这种数值化形式转化为可以捕捉语义信息的高维向量。这两个步骤共同构成了将原始的文本数据转化为机器学习模型可以处理的输入的过程。