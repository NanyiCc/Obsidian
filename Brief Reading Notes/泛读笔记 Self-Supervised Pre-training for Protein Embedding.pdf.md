
*Author:* Yuzhi Guo, Jiaxiang Wu, Hehuan Ma, Junzhou Huang

<span style="background-color: #efe3da">🔴Journal:</span>

<span style="background-color: #efe3da">🔴IF:</span>

***

## **<span style="color: red"><span style="background-color: #efe3da">📑 摘要</span></span>**

>In this paper, we propose a self-supervised pre-training model for learning structure embeddings from protein tertiary structures. Native protein structures are perturbed with random noise, and the pre-training model aims at estimating gradients over perturbed 3D structures. Specifically, we adopt SE(3)-invariant features as the model inputs and reconstruct gradients over 3D coordinates with SE(3)-equivariance preserved. Such a paradigm avoids the usage of sophisticated SE(3)-equivariant models, and dramatically improves the computational efficiency of pre-training models.

***

## **<span style="color: red"><span style="background-color: #efe3da">🎯 结论</span></span>**

>In this work, we propose a self-supervised pre-training model for protein structure. To the best of our knowledge, this is the first attempt to construct and evaluate self-supervised learning on protein 3D structures. In addition, our method can be easily applied to various downstream models. It is empirically demonstrated that our pre-training model can generate high quality structure embeddings for downstream tasks. Recent pre-training strategies mainly focus on the protein sequence dataset since it is easier to obtain and contains huge amount of data. However, even the dataset used for pre-training protein 3D structure is not as large as protein sequence dataset, we argue that the 3D structure contains more information than the sequence. In order to fully utilize the available protein data, our next move is to integrate the 3D structure pre-training strategy with a sequence-based pre-training method to acquire sufficient protein information.

***

## **<span style="color: red"><span style="background-color: #efe3da">❓是否需要精读</span></span>**

>

***

##
