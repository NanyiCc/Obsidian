图知识蒸馏相关文献GitHub：[GitHub - liujing1023/Graph-based-Knowledge-Distillation: Graph based Knowledge Distillation: A Survey](https://github.com/liujing1023/Graph-based-Knowledge-Distillation)

## 相关概念
### 传统深度学习中的知识蒸馏方法：
1. 目标蒸馏：利用教师模型的输出类别概率作为平滑标签来训练学生。
	1. 开山之作：[[1503.02531] Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
	2. 
2. 特征蒸馏： 利用教师网络结构中的中间层特征表示所包含的语义信息作为知识迁 移到学生模型中。目前已经成为主流，包括注意力机制的使用、特征空间的概率分布匹配，还有基于关系蒸馏的新方法
	1. 开山之作：[[1412.6550] FitNets: Hints for Thin Deep Nets](https://arxiv.org/abs/1412.6550)

### 基于图的知识蒸馏发展和分类
图知识蒸馏旨在将教师模型中直接/间接构造的样本关系语义信息蒸馏到学生模型中, 以获得更加通用的、丰富的、充分的知识。
![[Pasted image 20230925193356.png]]
