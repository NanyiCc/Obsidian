$$\begin{align} & p_\theta(x_{0:T}) := p(x_T)\prod^{T}_{t=1}p_\theta(x_{t-1}|x_t), \\ & p_\theta(x_{t-1}|x_t) := \mathcal{N}(x_{t-1};\mu_\theta(x_t,t),\sum_\theta(x_t,t)) \end{align}$$

这两个等式定义了一个扩散模型（diffusion model）中数据生成过程的概率分布。

让我们分开来解释一下这个公式的各个部分：

首先，我们有 $p_\theta(x_{0:T})$，这表示在模型参数 $\theta$ 下，从时间 0 到时间 T 的数据序列 \(x_{0:T}\) 的联合概率分布。

这个联合概率分布定义为 $p(x_T)$ 和 $\prod^{T}_{t=1}p_\theta(x_{t-1}|x_t)$ 的乘积。其中，$p(x_T)$ 是时间 T 的数据分布，通常被选择为某个固定的、已知的分布，例如高斯分布。而 $\prod^{T}_{t=1}p_\theta(x_{t-1}|x_t)$ 是从时间 T 到时间 1 的所有条件概率分布的乘积，这些条件概率分布表示在已知 $x_t$ 的情况下， $x_{t-1}$ 的概率分布。

然后，我们有 $p_\theta(x_{t-1}|x_t)$，这表示在模型参数 $\theta$ 下，已知 $x_t$ 的情况下，$x_{t-1}$ 的条件概率分布。这个分布被定义为一个高斯分布，其均值 $\mu$和协方差矩阵 $\Sigma$ 由函数 $\mu_\theta(x_t,t)$ 和 $\sum_\theta(x_t,t)$ 决定，这两个函数通常由神经网络参数化，输入是 $x_t$ 和时间步 t。

简而言之，这个公式定义了一个扩散模型，该模型开始于时间 T 的某个固定分布（例如高斯分布），然后通过一系列的逆向高斯过渡，生成从时间 T-1 到时间 0 的数据序列。每一步的过渡都由一个以当前数据和时间步为输入的神经网络决定。
- [[DDPM]]